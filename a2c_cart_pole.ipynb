{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8af4c7-05c9-4dc0-9fd0-21b5ad20c61b",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556a14c5-7b57-4b3b-aa5c-496c7e8623d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/env-2-rl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc34b6-91c6-4c50-8ea3-11b1496bec85",
   "metadata": {},
   "source": [
    "## Definition of the Actor and Critic classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004682b4-53c3-41ae-9608-8b93faeba676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_inputs, n_actions, hidden_size, learning_rate):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        policy = self.network(state)\n",
    "        return policy\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_inputs, hidden_size, learning_rate):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = self.network(state)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d1ead-d3b0-4500-afa8-5aacdf724045",
   "metadata": {},
   "source": [
    "## Definition of the A2C agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a08e616a-0a65-454a-9411-db8d2a208d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advantage_actor_critic(env,max_epochs, max_episodes, hidden_size, learning_rate, gamma):\n",
    "    n_inputs = env.observation_space.shape[0]\n",
    "    n_outputs = env.action_space.n\n",
    "    \n",
    "    actor = Actor(n_inputs, n_outputs, hidden_size, learning_rate)\n",
    "    critic = Critic(n_inputs, hidden_size, learning_rate)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    all_rewards = []\n",
    "    total_entropy = 0\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        log_probas = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        for epoch in range(max_epochs):\n",
    "            \n",
    "            #Actor critic decision\n",
    "            policy =  actor.forward(state)\n",
    "            policy_np = policy.detach().numpy() \n",
    "            value = critic.forward(state).detach().numpy()[0,0]                   \n",
    "            action = np.random.choice(n_outputs, p=np.squeeze(policy_np))\n",
    "            log_proba = torch.log(policy.squeeze(0)[action])\n",
    "            entropy = -np.sum(np.mean(policy_np) * np.log(policy_np))\n",
    "            \n",
    "            #Action towards new state\n",
    "            \n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            values.append(value)\n",
    "            log_probas.append(log_proba)\n",
    "            rewards.append(reward)            \n",
    "            total_entropy += entropy\n",
    "            \n",
    "            if done or epoch == max_epochs-1:\n",
    "                q_value = actor.forward(state).detach().numpy()[0,0]\n",
    "                sum_rewards = np.sum(rewards)\n",
    "                all_rewards.append(sum_rewards)\n",
    "\n",
    "                if episode % 10 == 0:                    \n",
    "                    print(f\"Episode: {episode}, total reward: {sum_rewards}\")  \n",
    "                break\n",
    "                \n",
    "        #Computation of the loss\n",
    "        values = torch.FloatTensor(values)\n",
    "        q_values = []\n",
    "        \n",
    "        for reward in (rewards[::-1]):\n",
    "            q_value = reward + gamma * q_value\n",
    "            q_values.append(q_value)\n",
    "            \n",
    "        q_values = torch.FloatTensor(q_values[::-1])\n",
    "        log_probas = torch.stack(log_probas)\n",
    "        \n",
    "        advantage = q_values -  values\n",
    "        actor_loss = torch.mean(-log_probas * advantage)\n",
    "        critic_loss = 1/2 * torch.mean(advantage**2)\n",
    "        actor_critic_loss = actor_loss + critic_loss + 0.001 * total_entropy\n",
    "        \n",
    "        #Optimization step\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        actor_critic_loss.backward()\n",
    "        \n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "    smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()  \n",
    "    return all_rewards, smoothed_rewards\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673916d4-bd71-4fd3-9451-a424d5846dfa",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a3bf24-9d3b-4035-8a44-094e00e0e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "max_episodes = 2000\n",
    "\n",
    "hidden_size = 256\n",
    "learning_rate = 0.0003\n",
    "gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462441d5-d0b2-45fc-82cc-57b94759fb67",
   "metadata": {},
   "source": [
    "## Train our agent in the CartPole environnment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4b697-cb1e-478b-ac32-0876feda6919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, total reward: 12.0\n",
      "Episode: 10, total reward: 11.0\n",
      "Episode: 20, total reward: 26.0\n",
      "Episode: 30, total reward: 70.0\n",
      "Episode: 40, total reward: 12.0\n",
      "Episode: 50, total reward: 12.0\n",
      "Episode: 60, total reward: 20.0\n",
      "Episode: 70, total reward: 18.0\n",
      "Episode: 80, total reward: 24.0\n",
      "Episode: 90, total reward: 25.0\n",
      "Episode: 100, total reward: 20.0\n",
      "Episode: 110, total reward: 25.0\n",
      "Episode: 120, total reward: 20.0\n",
      "Episode: 130, total reward: 11.0\n",
      "Episode: 140, total reward: 15.0\n",
      "Episode: 150, total reward: 17.0\n",
      "Episode: 160, total reward: 10.0\n",
      "Episode: 170, total reward: 20.0\n",
      "Episode: 180, total reward: 28.0\n",
      "Episode: 190, total reward: 17.0\n",
      "Episode: 200, total reward: 42.0\n",
      "Episode: 210, total reward: 34.0\n",
      "Episode: 220, total reward: 18.0\n",
      "Episode: 230, total reward: 73.0\n",
      "Episode: 240, total reward: 57.0\n",
      "Episode: 250, total reward: 14.0\n",
      "Episode: 260, total reward: 32.0\n",
      "Episode: 270, total reward: 16.0\n",
      "Episode: 280, total reward: 27.0\n",
      "Episode: 290, total reward: 69.0\n",
      "Episode: 300, total reward: 42.0\n",
      "Episode: 310, total reward: 16.0\n",
      "Episode: 320, total reward: 27.0\n",
      "Episode: 330, total reward: 23.0\n",
      "Episode: 340, total reward: 36.0\n",
      "Episode: 350, total reward: 29.0\n",
      "Episode: 360, total reward: 102.0\n",
      "Episode: 370, total reward: 43.0\n",
      "Episode: 380, total reward: 17.0\n",
      "Episode: 390, total reward: 32.0\n",
      "Episode: 400, total reward: 79.0\n",
      "Episode: 410, total reward: 14.0\n",
      "Episode: 420, total reward: 36.0\n",
      "Episode: 430, total reward: 27.0\n",
      "Episode: 440, total reward: 27.0\n",
      "Episode: 450, total reward: 32.0\n",
      "Episode: 460, total reward: 29.0\n",
      "Episode: 470, total reward: 71.0\n",
      "Episode: 480, total reward: 44.0\n",
      "Episode: 490, total reward: 76.0\n",
      "Episode: 500, total reward: 24.0\n",
      "Episode: 510, total reward: 34.0\n",
      "Episode: 520, total reward: 88.0\n",
      "Episode: 530, total reward: 47.0\n",
      "Episode: 540, total reward: 53.0\n",
      "Episode: 550, total reward: 29.0\n",
      "Episode: 560, total reward: 18.0\n",
      "Episode: 570, total reward: 39.0\n",
      "Episode: 580, total reward: 15.0\n",
      "Episode: 590, total reward: 44.0\n",
      "Episode: 600, total reward: 16.0\n",
      "Episode: 610, total reward: 38.0\n",
      "Episode: 620, total reward: 25.0\n",
      "Episode: 630, total reward: 52.0\n",
      "Episode: 640, total reward: 47.0\n",
      "Episode: 650, total reward: 105.0\n",
      "Episode: 660, total reward: 55.0\n",
      "Episode: 670, total reward: 58.0\n",
      "Episode: 680, total reward: 27.0\n",
      "Episode: 690, total reward: 58.0\n",
      "Episode: 700, total reward: 109.0\n",
      "Episode: 710, total reward: 163.0\n",
      "Episode: 720, total reward: 29.0\n",
      "Episode: 730, total reward: 40.0\n",
      "Episode: 740, total reward: 62.0\n",
      "Episode: 750, total reward: 46.0\n",
      "Episode: 760, total reward: 22.0\n",
      "Episode: 770, total reward: 171.0\n",
      "Episode: 780, total reward: 40.0\n",
      "Episode: 790, total reward: 22.0\n",
      "Episode: 800, total reward: 86.0\n",
      "Episode: 810, total reward: 28.0\n",
      "Episode: 820, total reward: 148.0\n",
      "Episode: 830, total reward: 65.0\n",
      "Episode: 840, total reward: 95.0\n",
      "Episode: 850, total reward: 39.0\n",
      "Episode: 860, total reward: 42.0\n",
      "Episode: 870, total reward: 142.0\n",
      "Episode: 880, total reward: 72.0\n",
      "Episode: 890, total reward: 42.0\n",
      "Episode: 900, total reward: 68.0\n",
      "Episode: 910, total reward: 124.0\n",
      "Episode: 920, total reward: 49.0\n",
      "Episode: 930, total reward: 63.0\n",
      "Episode: 940, total reward: 66.0\n",
      "Episode: 950, total reward: 117.0\n",
      "Episode: 960, total reward: 164.0\n",
      "Episode: 970, total reward: 106.0\n",
      "Episode: 980, total reward: 257.0\n",
      "Episode: 990, total reward: 141.0\n",
      "Episode: 1000, total reward: 150.0\n",
      "Episode: 1010, total reward: 197.0\n",
      "Episode: 1020, total reward: 19.0\n",
      "Episode: 1030, total reward: 37.0\n",
      "Episode: 1040, total reward: 290.0\n",
      "Episode: 1050, total reward: 78.0\n",
      "Episode: 1060, total reward: 222.0\n",
      "Episode: 1070, total reward: 267.0\n",
      "Episode: 1080, total reward: 112.0\n",
      "Episode: 1090, total reward: 106.0\n",
      "Episode: 1100, total reward: 98.0\n",
      "Episode: 1110, total reward: 138.0\n",
      "Episode: 1120, total reward: 192.0\n",
      "Episode: 1130, total reward: 52.0\n",
      "Episode: 1140, total reward: 300.0\n",
      "Episode: 1150, total reward: 176.0\n",
      "Episode: 1160, total reward: 119.0\n",
      "Episode: 1170, total reward: 171.0\n",
      "Episode: 1180, total reward: 216.0\n",
      "Episode: 1190, total reward: 143.0\n",
      "Episode: 1200, total reward: 18.0\n",
      "Episode: 1210, total reward: 121.0\n",
      "Episode: 1220, total reward: 209.0\n",
      "Episode: 1230, total reward: 140.0\n",
      "Episode: 1240, total reward: 171.0\n",
      "Episode: 1250, total reward: 300.0\n",
      "Episode: 1260, total reward: 160.0\n",
      "Episode: 1270, total reward: 42.0\n",
      "Episode: 1280, total reward: 111.0\n",
      "Episode: 1290, total reward: 192.0\n",
      "Episode: 1300, total reward: 285.0\n",
      "Episode: 1310, total reward: 300.0\n",
      "Episode: 1320, total reward: 171.0\n",
      "Episode: 1330, total reward: 300.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "rewards, smoothed_rewards = advantage_actor_critic(env,max_epochs, max_episodes, hidden_size, learning_rate, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c264f-a3bd-44d7-a705-90551fb3e496",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7f189-636f-467f-8f10-43fa499abc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f406e1b-8add-41c6-8694-c7c66fb7cd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env-2-rl]",
   "language": "python",
   "name": "conda-env-env-2-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
